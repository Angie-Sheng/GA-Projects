{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "In this part, I would like to scrape more fake news to enlarge my data sets. Simply, I use The pushshift.io Reddit API to help me extract more than thousands of onion news posted in Reddit.But, after analysing the data, I found poor quality due to missing content can be a barrier to use these data, so I didn't include this part into my final project. But, I will still list the process here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# API scrape \n",
    "from psaw import PushshiftAPI\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(subreddit):\n",
    "    \n",
    "    # Instantiate \n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    # Create list of scraped data\n",
    "    scrape_list = list(api.search_submissions(subreddit=subreddit, filter=['title','selftext', 'subreddit', 'num_comments', 'author', 'subreddit_subscribers', 'score', 'domain', 'created_utc'],limit=15000))\n",
    "\n",
    "    #Filter list to only show Subreddit titles and Subreddit category \n",
    "    clean_scrape_lst = []\n",
    "    for i in range(len(scrape_list)):\n",
    "        scrape_dict = {}\n",
    "        scrape_dict['author'] = scrape_list[i][0]\n",
    "        scrape_dict['timestamp'] = scrape_list[i][1]        \n",
    "        scrape_dict['domain'] = scrape_list[i][2]\n",
    "        scrape_dict['num_comments'] = scrape_list[i][3]\n",
    "        scrape_dict['score'] = scrape_list[i][4]  \n",
    "        scrape_dict['selftext'] = scrape_list[i][5]         \n",
    "        scrape_dict['subreddit'] = scrape_list[i][6]\n",
    "        scrape_dict['title'] = scrape_list[i][8]\n",
    "        clean_scrape_lst.append(scrape_dict)\n",
    "\n",
    "    # Show number of subscribers\n",
    "    print(subreddit, 'subscribers:',scrape_list[1][6])\n",
    "    \n",
    "    # Return list of scraped data\n",
    "    return clean_scrape_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theonion subscribers: TheOnion\n",
      "df_onion shape: (15000, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aresef</td>\n",
       "      <td>sports.theonion.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567026670</td>\n",
       "      <td>Case Keenum Wins Redskins Starting Job With He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aresef</td>\n",
       "      <td>politics.theonion.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567023428</td>\n",
       "      <td>House Wayans And Means Committee Approves $50 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProbablyDrDre</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567019827</td>\n",
       "      <td>Expert On Anteaters Wasted Entire Life Studyin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dwaxe</td>\n",
       "      <td>clickhole.com</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567018342</td>\n",
       "      <td>Celebrating An Icon: President Trump Has Invit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dwaxe</td>\n",
       "      <td>lifestyle.clickhole.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567016551</td>\n",
       "      <td>Inspiring: This Man Just Became The Oldest Per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                   domain  num_comments  score selftext  \\\n",
       "0         aresef      sports.theonion.com             0      1            \n",
       "1         aresef    politics.theonion.com             0      1            \n",
       "2  ProbablyDrDre              youtube.com             0      1            \n",
       "3          dwaxe            clickhole.com             2      1            \n",
       "4          dwaxe  lifestyle.clickhole.com             0      1            \n",
       "\n",
       "  subreddit   timestamp                                              title  \n",
       "0  TheOnion  1567026670  Case Keenum Wins Redskins Starting Job With He...  \n",
       "1  TheOnion  1567023428  House Wayans And Means Committee Approves $50 ...  \n",
       "2  TheOnion  1567019827  Expert On Anteaters Wasted Entire Life Studyin...  \n",
       "3  TheOnion  1567018342  Celebrating An Icon: President Trump Has Invit...  \n",
       "4  TheOnion  1567016551  Inspiring: This Man Just Became The Oldest Per...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Call function and create DataFrame\n",
    "df_onion = pd.DataFrame(scrape_data('theonion'))\n",
    "\n",
    "# Save data to csv\n",
    "df_onion.to_csv('./the_onion.csv')\n",
    "\n",
    "# Shape of DataFrame\n",
    "print(f'df_onion shape: {df_onion.shape}')\n",
    "\n",
    "# Show head\n",
    "df_onion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onion = pd.read_csv('./the_onion.csv',index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aresef</td>\n",
       "      <td>sports.theonion.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567026670</td>\n",
       "      <td>Case Keenum Wins Redskins Starting Job With He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aresef</td>\n",
       "      <td>politics.theonion.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567023428</td>\n",
       "      <td>House Wayans And Means Committee Approves $50 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProbablyDrDre</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567019827</td>\n",
       "      <td>Expert On Anteaters Wasted Entire Life Studyin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dwaxe</td>\n",
       "      <td>clickhole.com</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567018342</td>\n",
       "      <td>Celebrating An Icon: President Trump Has Invit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dwaxe</td>\n",
       "      <td>lifestyle.clickhole.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567016551</td>\n",
       "      <td>Inspiring: This Man Just Became The Oldest Per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                   domain  num_comments  score selftext  \\\n",
       "0         aresef      sports.theonion.com             0      1      NaN   \n",
       "1         aresef    politics.theonion.com             0      1      NaN   \n",
       "2  ProbablyDrDre              youtube.com             0      1      NaN   \n",
       "3          dwaxe            clickhole.com             2      1      NaN   \n",
       "4          dwaxe  lifestyle.clickhole.com             0      1      NaN   \n",
       "\n",
       "  subreddit   timestamp                                              title  \n",
       "0  TheOnion  1567026670  Case Keenum Wins Redskins Starting Job With He...  \n",
       "1  TheOnion  1567023428  House Wayans And Means Committee Approves $50 ...  \n",
       "2  TheOnion  1567019827  Expert On Anteaters Wasted Entire Life Studyin...  \n",
       "3  TheOnion  1567018342  Celebrating An Icon: President Trump Has Invit...  \n",
       "4  TheOnion  1567016551  Inspiring: This Man Just Became The Oldest Per...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_data(dataframe,column):\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    dataframe.drop_duplicates(subset=column, inplace=True)\n",
    "    \n",
    "    # Remove punctation\n",
    "    dataframe['title'] = dataframe['title'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "    # Remove numbers \n",
    "    dataframe['title'] = dataframe['title'].str.replace('[^A-Za-z]',' ')\n",
    "\n",
    "    # Make sure any double-spaces are single \n",
    "    dataframe['title'] = dataframe['title'].str.replace('  ',' ')\n",
    "    dataframe['title'] = dataframe['title'].str.replace('  ',' ')\n",
    "\n",
    "    # Transform all text to lowercase\n",
    "    dataframe['title'] = dataframe['title'].str.lower()\n",
    "    \n",
    "    print(\"New shape:\", dataframe.shape)\n",
    "    return dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape: (14257, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aresef</td>\n",
       "      <td>sports.theonion.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567026670</td>\n",
       "      <td>case keenum wins redskins starting job with he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aresef</td>\n",
       "      <td>politics.theonion.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567023428</td>\n",
       "      <td>house wayans and means committee approves  mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ProbablyDrDre</td>\n",
       "      <td>youtube.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567019827</td>\n",
       "      <td>expert on anteaters wasted entire life studyin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dwaxe</td>\n",
       "      <td>clickhole.com</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567018342</td>\n",
       "      <td>celebrating an icon president trump has invite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dwaxe</td>\n",
       "      <td>lifestyle.clickhole.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TheOnion</td>\n",
       "      <td>1567016551</td>\n",
       "      <td>inspiring this man just became the oldest pers...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author                   domain  num_comments  score selftext  \\\n",
       "0         aresef      sports.theonion.com             0      1      NaN   \n",
       "1         aresef    politics.theonion.com             0      1      NaN   \n",
       "2  ProbablyDrDre              youtube.com             0      1      NaN   \n",
       "3          dwaxe            clickhole.com             2      1      NaN   \n",
       "4          dwaxe  lifestyle.clickhole.com             0      1      NaN   \n",
       "\n",
       "  subreddit   timestamp                                              title  \n",
       "0  TheOnion  1567026670  case keenum wins redskins starting job with he...  \n",
       "1  TheOnion  1567023428  house wayans and means committee approves  mil...  \n",
       "2  TheOnion  1567019827  expert on anteaters wasted entire life studyin...  \n",
       "3  TheOnion  1567018342  celebrating an icon president trump has invite...  \n",
       "4  TheOnion  1567016551  inspiring this man just became the oldest pers...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(df_onion,'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, '[deleted]', 'TheOnion',\n",
       "       \"As all of you who have tried to visit in the last couple days saw, the subreddit has been private. We closed it to show solidarity with the other subs who closed. While you may not agree with the strike or the fact that we closed, we appreciate having all of you here. Hopefully some good can come of the strike and everything won't be in vain.\\n\\nBut once again, thank you for sticking with us through this. Sorry that petty reddit politics has affected your ability to browse here. If you have any suggestions or opinions express them here or message us. \\n\\nThanks,\\n\\n/r/TheOnion mods\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_onion.selftext.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheOnion start date: 2015-06-09 20:17:33\n",
      "TheOnion end date: 2019-08-28 21:11:10\n"
     ]
    }
   ],
   "source": [
    "# Convert Unix Timestamp to Datetime\n",
    "df_onion['timestamp'] = pd.to_datetime(df_onion['timestamp'], unit='s')\n",
    "\n",
    "# Show date-range of posts scraped from r/TheOnion and r/nottheonion\n",
    "print(\"TheOnion start date:\", df_onion['timestamp'].min())\n",
    "print(\"TheOnion end date:\", df_onion['timestamp'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
